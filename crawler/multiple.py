"""
LinkedIn Profile Scraper - Multiple URLs with RabbitMQ
Auto-load URLs from profile/*.json files and process with workers
"""
import json
import glob
import os
import threading
import time
from crawler import LinkedInCrawler
from helper.rabbitmq_helper import RabbitMQManager, ack_message, nack_message
from main import save_profile_data


# Statistics
stats = {
    'processing': 0,
    'completed': 0,
    'failed': 0,
    'skipped': 0,
    'lock': threading.Lock()
}


def print_stats():
    """Print current statistics"""
    print("\n" + "="*60)
    print("STATISTICS")
    print("="*60)
    print(f"Processing: {stats['processing']}")
    print(f"Completed: {stats['completed']}")
    print(f"Failed: {stats['failed']}")
    print(f"Skipped: {stats['skipped']}")
    if stats['completed'] + stats['failed'] > 0:
        success_rate = stats['completed'] / (stats['completed'] + stats['failed']) * 100
        print(f"Success Rate: {success_rate:.1f}%")
    print("="*60)


def load_urls_from_profile_folder():
    """Load all URLs from profile/*.json files"""
    urls = []
    skipped = 0
    
    # Get all JSON files in profile folder
    json_files = glob.glob('profile/*.json')
    
    if not json_files:
        print("âš  No JSON files found in profile/ folder")
        return urls, skipped
    
    print(f"â†’ Found {len(json_files)} JSON file(s) in profile/")
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # Handle both array and single object
                if isinstance(data, list):
                    profiles = data
                else:
                    profiles = [data]
                
                for profile in profiles:
                    url = profile.get('profile_url', '')
                    
                    if not url:
                        continue
                    
                    # Skip URLs with "sales" in them
                    if '/sales/' in url.lower():
                        skipped += 1
                        print(f"  âŠ˜ Skipped (sales URL): {profile.get('name', 'Unknown')}")
                        continue
                    
                    urls.append(url)
                    print(f"  âœ“ Added: {profile.get('name', 'Unknown')}")
        
        except Exception as e:
            print(f"  âœ— Error reading {json_file}: {e}")
            continue
    
    return urls, skipped


def worker_thread(worker_id, mq_config):
    """Worker thread that continuously processes messages"""
    print(f"[Worker {worker_id}] Started")
    
    # Each worker has its own RabbitMQ connection
    mq = RabbitMQManager()
    mq.host = mq_config['host']
    mq.port = mq_config['port']
    mq.username = mq_config['username']
    mq.password = mq_config['password']
    mq.queue_name = mq_config['queue_name']
    
    if not mq.connect():
        print(f"[Worker {worker_id}] Failed to connect to RabbitMQ")
        return
    
    # Set QoS - only process 1 message at a time
    mq.channel.basic_qos(prefetch_count=1)
    
    def callback(ch, method, properties, body):
        """Process each message"""
        crawler = None
        
        try:
            # Parse message
            message = json.loads(body)
            url = message.get('url')
            
            if not url:
                print(f"[Worker {worker_id}] âœ— Invalid message")
                ack_message(ch, method.delivery_tag)
                return
            
            print(f"\n[Worker {worker_id}] ğŸ“¥ Processing: {url}")
            
            with stats['lock']:
                stats['processing'] += 1
            
            # Create crawler and process
            crawler = LinkedInCrawler()
            
            try:
                # Login (will use cookies if available)
                crawler.login()
                
                # Scrape profile
                profile_data = crawler.get_profile(url)
                
                # Save to file
                save_profile_data(profile_data)
                
                with stats['lock']:
                    stats['completed'] += 1
                
                print(f"[Worker {worker_id}] âœ“ Completed: {profile_data.get('name', 'Unknown')}")
                
                # Print stats after completion
                print_stats()
                
                # Acknowledge message
                ack_message(ch, method.delivery_tag)
                
            except Exception as e:
                with stats['lock']:
                    stats['failed'] += 1
                
                print(f"[Worker {worker_id}] âœ— Error: {e}")
                
                # Print stats after failure
                print_stats()
                
                # Don't requeue to avoid infinite loop
                nack_message(ch, method.delivery_tag, requeue=False)
            
            finally:
                # Close browser
                if crawler:
                    crawler.close()
                
                with stats['lock']:
                    stats['processing'] -= 1
        
        except Exception as e:
            print(f"[Worker {worker_id}] âœ— Fatal error: {e}")
            nack_message(ch, method.delivery_tag, requeue=False)
    
    try:
        # Start consuming
        mq.channel.basic_consume(
            queue=mq.queue_name,
            on_message_callback=callback,
            auto_ack=False
        )
        
        print(f"[Worker {worker_id}] Waiting for messages...")
        mq.channel.start_consuming()
    
    except Exception as e:
        print(f"[Worker {worker_id}] Error: {e}")
    
    finally:
        mq.close()
        print(f"[Worker {worker_id}] Stopped")


def main():
    print("="*60)
    print("LINKEDIN SCRAPER - AUTO MODE")
    print("="*60)
    
    # Default 3 workers
    num_workers = 3
    print(f"â†’ Using {num_workers} workers (default)")
    
    # Load URLs from profile folder
    print("\nâ†’ Loading URLs from profile/*.json files...")
    urls, skipped_count = load_urls_from_profile_folder()
    
    if not urls:
        print("\nâœ— No valid URLs found!")
        print("  Make sure you have JSON files in profile/ folder")
        print("  URLs with '/sales/' are automatically skipped")
        return
    
    print(f"\nâ†’ Summary:")
    print(f"  - Valid URLs: {len(urls)}")
    print(f"  - Skipped (sales): {skipped_count}")
    
    stats['skipped'] = skipped_count
    
    # Connect to RabbitMQ
    print("\nâ†’ Connecting to RabbitMQ...")
    mq = RabbitMQManager()
    if not mq.connect():
        print("âœ— Failed to connect to RabbitMQ. Is it running?")
        print("\nTo start RabbitMQ:")
        print("  docker-compose up -d")
        return
    
    # Publish URLs to queue
    print(f"\nâ†’ Publishing {len(urls)} URLs to queue...")
    success_count = mq.publish_urls(urls)
    
    if success_count == 0:
        print("âœ— Failed to publish URLs")
        mq.close()
        return
    
    # Show queue status
    queue_size = mq.get_queue_size()
    print(f"\nâ†’ Queue status:")
    print(f"  - Messages in queue: {queue_size}")
    print(f"  - Queue name: {mq.queue_name}")
    
    # Save config for workers
    mq_config = {
        'host': mq.host,
        'port': mq.port,
        'username': mq.username,
        'password': mq.password,
        'queue_name': mq.queue_name
    }
    
    mq.close()
    
    print(f"\nâ†’ Starting {num_workers} workers...")
    print("  Workers will process URLs automatically")
    print("  Press Ctrl+C to stop")
    print(f"  Management UI: http://localhost:15672 (guest/guest)")
    
    # Start worker threads
    threads = []
    for i in range(num_workers):
        t = threading.Thread(target=worker_thread, args=(i+1, mq_config), daemon=True)
        t.start()
        threads.append(t)
        time.sleep(0.5)
    
    print(f"\nâœ“ All {num_workers} workers are running!")
    
    try:
        # Keep main thread alive
        while True:
            time.sleep(1)
    
    except KeyboardInterrupt:
        print("\n\nâš  Interrupted by user. Stopping all workers...")
        print("  (Workers will finish current tasks)")
    
    finally:
        # Wait a bit for workers to finish current tasks
        time.sleep(3)
        
        # Final stats
        print("\n" + "="*60)
        print("FINAL RESULTS")
        print("="*60)
        print(f"Total URLs: {len(urls)}")
        print(f"âœ“ Completed: {stats['completed']}")
        print(f"âœ— Failed: {stats['failed']}")
        print(f"âŠ˜ Skipped (sales): {stats['skipped']}")
        if stats['completed'] + stats['failed'] > 0:
            success_rate = stats['completed'] / (stats['completed'] + stats['failed']) * 100
            print(f"ğŸ“Š Success Rate: {success_rate:.1f}%")
        print("="*60)
        print(f"\nOutput directory: data/output/")


if __name__ == "__main__":
    main()
